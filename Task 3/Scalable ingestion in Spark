from pyspark.sql import functions as F

# --- 2) Load matches sample (strings first), then limit rows for DEV ---
# Read as strings to keep it light/stable

for x in part:

    SAMPLE_CSV = OUTPUT_ROOT / "matches_parts" / f"part-{x}.csv"
    assert SAMPLE_CSV.exists(), f"Missing sample: {SAMPLE_CSV}.csv" 
    
    print(f"Viewing part-{x}")
    matches_raw = (spark.read.option("header","true").option("inferSchema","false")
               .csv(str(SAMPLE_CSV))
               # project only what we need for cleaning/EDA/viz
               .select("match_id","start_time","duration","radiant_win","cluster","game_mode","lobby_type")
              )

    # Limit rows for development (keeps downstream joins/EDA fast)
    matches_raw = matches_raw.limit(DEV_N_MATCHES).cache()


    print("âœ… DataFrames created (dev-limited)")
    print("matches_raw columns:", matches_raw.columns)


    # Tiny preview (one light action)
    matches_raw.limit(5).show(truncate=False)
